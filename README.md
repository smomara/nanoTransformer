# nanoTransformer
## Overview
This project is an implementation of a small decoding transformer basd on the groundbreaking paper !["Attention is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al., while following the guidance provided in Andrej Karpathy's video !["Let's build GPT"](https://www.youtube.com/watch?v=kCc8FmEb1nY). The goal of the project is to gain a deeper understanding of transformer architecture and how LLMs are built.

## Future Work
In the future, I am to:
- <b>Scale Up the Model</b>: Access stronger computing power to scale up the decoding transformer
- <b>Implement Encoding</b>: Extend the project by incorporating the encoding component of the transformer architecture
- <b>Fine-Tuning for Prompt Responses</b>: Explore fine-tuning techniques so the model can generate meaningful responses to specific prompts
- <b>Explore Bleeding Edge Models</b>: Investigate advanced models such as the Mixture of Experts

These steps aim to advance the nanoTransformer project, making it more robust and versatile. It will be difficult to do given my current access to computing power, but using innovative tech at any scale is fun.
