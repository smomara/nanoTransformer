# nanoTransformer
## Overview
This project is an implementation of a small decoding transformer basd on the groundbreaking paper !["Attention is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al., while following the guidance provided in Andrej Karpathy's video !["Let's build GPT"](https://www.youtube.com/watch?v=kCc8FmEb1nY). The goal of the project is to gain a deeper understanding of transformer architecture and how LLMs are built.
